{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IOA Aletheia Runtime Demo - Configuration\n",
        "# Set to True to use real LLM providers (requires API keys)\n",
        "# Set to False for zero-setup simulation mode (default)\n",
        "USE_REAL_LLM = False\n",
        "\n",
        "print(\"üéØ IOA Aletheia Runtime Demo\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"üìä Mode: {'Real LLM Providers' if USE_REAL_LLM else 'Simulation Mode (No Keys Required)'}\")\n",
        "print(f\"üîë API Keys: {'Required' if USE_REAL_LLM else 'Not Required'}\")\n",
        "print(\"=\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "!pip install --quiet matplotlib pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and load built-in Aletheia policies\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Mock Aletheia policy configurations\n",
        "aletheia_policies = {\n",
        "    \"bias_shadow@v1\": {\n",
        "        \"facet\": \"Transparency\",\n",
        "        \"weight\": 0.25,\n",
        "        \"threshold\": 0.8,\n",
        "        \"description\": \"Bias detection and shadow testing for transparency\"\n",
        "    },\n",
        "    \"sox-lite@v1\": {\n",
        "        \"facet\": \"Compliance\",\n",
        "        \"weight\": 0.20,\n",
        "        \"threshold\": 0.85,\n",
        "        \"description\": \"SOX compliance monitoring and reporting\"\n",
        "    },\n",
        "    \"security-audit@v1\": {\n",
        "        \"facet\": \"Security\",\n",
        "        \"weight\": 0.20,\n",
        "        \"threshold\": 0.75,\n",
        "        \"description\": \"Security audit and vulnerability assessment\"\n",
        "    },\n",
        "    \"reliability-check@v1\": {\n",
        "        \"facet\": \"Reliability\",\n",
        "        \"weight\": 0.15,\n",
        "        \"threshold\": 0.8,\n",
        "        \"description\": \"System reliability and fault tolerance checks\"\n",
        "    },\n",
        "    \"ethics-governance@v1\": {\n",
        "        \"facet\": \"Ethics\",\n",
        "        \"weight\": 0.15,\n",
        "        \"threshold\": 0.8,\n",
        "        \"description\": \"Ethical AI governance and decision monitoring\"\n",
        "    },\n",
        "    \"sustainability-metrics@v1\": {\n",
        "        \"facet\": \"Sustainability\",\n",
        "        \"weight\": 0.05,\n",
        "        \"threshold\": 0.7,\n",
        "        \"description\": \"Environmental impact and sustainability tracking\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Aletheia policies loaded successfully!\")\n",
        "print(f\"üìä Total policies: {len(aletheia_policies)}\")\n",
        "print(\"\\nüîç Policy Summary:\")\n",
        "for policy_id, config in aletheia_policies.items():\n",
        "    print(f\"  ‚Ä¢ {policy_id}: {config['facet']} (weight: {config['weight']})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Real LLM Provider Integration (only runs if USE_REAL_LLM = True)\n",
        "if USE_REAL_LLM:\n",
        "    print(\"üîß Setting up real LLM providers...\")\n",
        "    \n",
        "    # Install provider SDKs only when needed\n",
        "    import subprocess\n",
        "    import sys\n",
        "    \n",
        "    providers = {\n",
        "        \"openai\": \"openai\",\n",
        "        \"anthropic\": \"anthropic\", \n",
        "        \"gemini\": \"google-generativeai\",\n",
        "        \"huggingface\": \"transformers\"\n",
        "    }\n",
        "    \n",
        "    # Install required packages\n",
        "    for provider, package in providers.items():\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n",
        "            print(f\"‚úÖ Installed {package}\")\n",
        "        except:\n",
        "            print(f\"‚ö†Ô∏è Could not install {package}\")\n",
        "    \n",
        "    # Provider selection and API key input\n",
        "    print(\"\\nü§ñ Select your LLM provider:\")\n",
        "    print(\"1. OpenAI (GPT-4)\")\n",
        "    print(\"2. Anthropic (Claude)\")\n",
        "    print(\"3. Google (Gemini)\")\n",
        "    print(\"4. Hugging Face (Local)\")\n",
        "    \n",
        "    provider_choice = input(\"Enter choice (1-4): \").strip()\n",
        "    provider_map = {\"1\": \"openai\", \"2\": \"anthropic\", \"3\": \"gemini\", \"4\": \"huggingface\"}\n",
        "    selected_provider = provider_map.get(provider_choice, \"openai\")\n",
        "    \n",
        "    print(f\"\\nüîë Enter your {selected_provider.upper()} API key:\")\n",
        "    api_key = input(\"API Key: \").strip()\n",
        "    \n",
        "    if not api_key:\n",
        "        print(\"‚ö†Ô∏è No API key provided. Falling back to simulation mode.\")\n",
        "        USE_REAL_LLM = False\n",
        "    else:\n",
        "        print(f\"‚úÖ {selected_provider.upper()} API key configured\")\n",
        "        \n",
        "        # Test the provider with a simple call\n",
        "        try:\n",
        "            if selected_provider == \"openai\":\n",
        "                import openai\n",
        "                openai.api_key = api_key\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": \"Hello, this is a test.\"}],\n",
        "                    max_tokens=10\n",
        "                )\n",
        "                test_output = response.choices[0].message.content\n",
        "                \n",
        "            elif selected_provider == \"anthropic\":\n",
        "                import anthropic\n",
        "                client = anthropic.Anthropic(api_key=api_key)\n",
        "                response = client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=10,\n",
        "                    messages=[{\"role\": \"user\", \"content\": \"Hello, this is a test.\"}]\n",
        "                )\n",
        "                test_output = response.content[0].text\n",
        "                \n",
        "            elif selected_provider == \"gemini\":\n",
        "                import google.generativeai as genai\n",
        "                genai.configure(api_key=api_key)\n",
        "                model = genai.GenerativeModel('gemini-pro')\n",
        "                response = model.generate_content(\"Hello, this is a test.\")\n",
        "                test_output = response.text\n",
        "                \n",
        "            elif selected_provider == \"huggingface\":\n",
        "                from transformers import pipeline\n",
        "                generator = pipeline('text-generation', model='gpt2', max_length=20)\n",
        "                response = generator(\"Hello, this is a test.\")\n",
        "                test_output = response[0]['generated_text']\n",
        "            \n",
        "            print(f\"‚úÖ Provider test successful: {test_output[:50]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Provider test failed: {str(e)}\")\n",
        "            print(\"‚ö†Ô∏è Falling back to simulation mode.\")\n",
        "            USE_REAL_LLM = False\n",
        "else:\n",
        "    print(\"üìä Using simulation mode - no real LLM calls will be made\")\n",
        "    selected_provider = None\n",
        "    api_key = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Runtime governance simulation (supports both mock and real LLM)\n",
        "import random\n",
        "import time\n",
        "\n",
        "def call_real_llm(prompt, provider, api_key):\n",
        "    \"\"\"Call real LLM provider with the given prompt.\"\"\"\n",
        "    try:\n",
        "        if provider == \"openai\":\n",
        "            import openai\n",
        "            openai.api_key = api_key\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=100\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "            \n",
        "        elif provider == \"anthropic\":\n",
        "            import anthropic\n",
        "            client = anthropic.Anthropic(api_key=api_key)\n",
        "            response = client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=100,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            return response.content[0].text\n",
        "            \n",
        "        elif provider == \"gemini\":\n",
        "            import google.generativeai as genai\n",
        "            genai.configure(api_key=api_key)\n",
        "            model = genai.GenerativeModel('gemini-pro')\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text\n",
        "            \n",
        "        elif provider == \"huggingface\":\n",
        "            from transformers import pipeline\n",
        "            generator = pipeline('text-generation', model='gpt2', max_length=50)\n",
        "            response = generator(prompt)\n",
        "            return response[0]['generated_text']\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è LLM call failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def simulate_ioa_runtime_governance(prompt, context, use_real_llm=False, provider=None, api_key=None):\n",
        "    \"\"\"Simulate IOA runtime governance with Aletheia facet scoring.\"\"\"\n",
        "    \n",
        "    # Input processing\n",
        "    print(f\"üéØ Processing request: {prompt[:50]}...\")\n",
        "    print(f\"üìã Context: {context}\")\n",
        "    print(f\"ü§ñ LLM Mode: {'Real Provider' if use_real_llm else 'Simulation'}\")\n",
        "    \n",
        "    # Get LLM response (real or simulated)\n",
        "    if use_real_llm and provider and api_key:\n",
        "        print(\"üìû Calling real LLM provider...\")\n",
        "        llm_response = call_real_llm(prompt, provider, api_key)\n",
        "        if llm_response is None:\n",
        "            print(\"‚ö†Ô∏è Real LLM failed, falling back to simulation\")\n",
        "            use_real_llm = False\n",
        "    else:\n",
        "        llm_response = f\"Simulated response for: {prompt[:30]}...\"\n",
        "    \n",
        "    # Simulate policy enforcement delay\n",
        "    time.sleep(0.1)\n",
        "    \n",
        "    # Facet scoring (enhanced for real LLM responses)\n",
        "    facet_scores = {}\n",
        "    for policy_id, config in aletheia_policies.items():\n",
        "        # Base scoring\n",
        "        base_score = random.uniform(0.6, 0.95)\n",
        "        \n",
        "        # Adjust based on content type\n",
        "        if \"financial\" in prompt.lower():\n",
        "            base_score += 0.1\n",
        "        if \"medical\" in prompt.lower():\n",
        "            base_score += 0.15\n",
        "            \n",
        "        # Real LLM responses get slight boost in certain facets\n",
        "        if use_real_llm and llm_response:\n",
        "            if \"transparency\" in config['facet'].lower():\n",
        "                base_score += 0.05  # Real responses tend to be more transparent\n",
        "            if \"ethics\" in config['facet'].lower():\n",
        "                base_score += 0.03  # Real responses have better ethical consideration\n",
        "        \n",
        "        facet_scores[config['facet']] = min(1.0, max(0.0, base_score))\n",
        "    \n",
        "    # Calculate overall governance score\n",
        "    weighted_score = sum(\n",
        "        score * aletheia_policies[policy_id]['weight']\n",
        "        for policy_id, config in aletheia_policies.items()\n",
        "        for score in [facet_scores[config['facet']]]\n",
        "    )\n",
        "    \n",
        "    # Determine if request passes governance\n",
        "    passes_governance = weighted_score >= 0.8\n",
        "    \n",
        "    return {\n",
        "        \"facet_scores\": facet_scores,\n",
        "        \"weighted_score\": weighted_score,\n",
        "        \"passes_governance\": passes_governance,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"llm_response\": llm_response,\n",
        "        \"llm_mode\": \"real\" if use_real_llm else \"simulation\",\n",
        "        \"evidence_bundle\": {\n",
        "            \"bundle_id\": f\"ioa_{int(time.time())}\",\n",
        "            \"policies_applied\": list(aletheia_policies.keys()),\n",
        "            \"decision\": \"APPROVED\" if passes_governance else \"BLOCKED\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Test with sample prompts\n",
        "test_prompts = [\n",
        "    \"Analyze this financial data for investment recommendations\",\n",
        "    \"Provide medical diagnosis based on patient symptoms\", \n",
        "    \"Generate creative content for marketing campaign\",\n",
        "    \"Process customer data for personalization\"\n",
        "]\n",
        "\n",
        "print(\"üöÄ IOA Runtime Governance Simulation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Run tests with current configuration\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüìù Test Case {i}: {prompt}\")\n",
        "    result = simulate_ioa_runtime_governance(\n",
        "        prompt, \n",
        "        {\"user_id\": f\"demo_user_{i}\", \"test\": True},\n",
        "        use_real_llm=USE_REAL_LLM,\n",
        "        provider=selected_provider if USE_REAL_LLM else None,\n",
        "        api_key=api_key if USE_REAL_LLM else None\n",
        "    )\n",
        "    \n",
        "    print(f\"üìä Governance Score: {result['weighted_score']:.3f}\")\n",
        "    print(f\"‚úÖ Decision: {result['evidence_bundle']['decision']}\")\n",
        "    print(f\"üÜî Evidence Bundle: {result['evidence_bundle']['bundle_id']}\")\n",
        "    print(f\"ü§ñ LLM Mode: {result['llm_mode']}\")\n",
        "    if result['llm_response']:\n",
        "        print(f\"üí¨ LLM Response: {result['llm_response'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Mock vs Real LLM Comparison (only shows if real LLM was used)\n",
        "if USE_REAL_LLM and selected_provider and api_key:\n",
        "    print(\"üìä Mock vs Real LLM Comparison\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Run same test with both modes\n",
        "    test_prompt = \"Analyze this financial data for investment recommendations\"\n",
        "    \n",
        "    print(\"üîÑ Running comparison test...\")\n",
        "    \n",
        "    # Mock result\n",
        "    mock_result = simulate_ioa_runtime_governance(\n",
        "        test_prompt,\n",
        "        {\"user_id\": \"comparison_test\", \"test\": True},\n",
        "        use_real_llm=False\n",
        "    )\n",
        "    \n",
        "    # Real result  \n",
        "    real_result = simulate_ioa_runtime_governance(\n",
        "        test_prompt,\n",
        "        {\"user_id\": \"comparison_test\", \"test\": True},\n",
        "        use_real_llm=True,\n",
        "        provider=selected_provider,\n",
        "        api_key=api_key\n",
        "    )\n",
        "    \n",
        "    # Create comparison table\n",
        "    import pandas as pd\n",
        "    \n",
        "    comparison_data = []\n",
        "    for facet in mock_result['facet_scores'].keys():\n",
        "        comparison_data.append({\n",
        "            'Facet': facet,\n",
        "            'Mock Score': f\"{mock_result['facet_scores'][facet]:.3f}\",\n",
        "            'Real Score': f\"{real_result['facet_scores'][facet]:.3f}\",\n",
        "            'Delta': f\"{real_result['facet_scores'][facet] - mock_result['facet_scores'][facet]:+.3f}\"\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\nüìã Facet Score Comparison:\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    print(f\"\\nüìä Overall Score Comparison:\")\n",
        "    print(f\"Mock:  {mock_result['weighted_score']:.3f}\")\n",
        "    print(f\"Real:  {real_result['weighted_score']:.3f}\")\n",
        "    print(f\"Delta: {real_result['weighted_score'] - mock_result['weighted_score']:+.3f}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Decision Comparison:\")\n",
        "    print(f\"Mock:  {mock_result['evidence_bundle']['decision']}\")\n",
        "    print(f\"Real:  {real_result['evidence_bundle']['decision']}\")\n",
        "    \n",
        "    print(f\"\\nüí¨ Real LLM Response Preview:\")\n",
        "    print(f\"{real_result['llm_response'][:200]}...\")\n",
        "    \n",
        "else:\n",
        "    print(\"üìä Comparison mode not available (simulation mode only)\")\n",
        "    print(\"üí° Set USE_REAL_LLM = True and provide API key to see mock vs real comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Providers & Costs\n",
        "\n",
        "**Free Tier Options:**\n",
        "- **OpenAI**: $5 free credit (new accounts) - [Sign up](https://platform.openai.com/)\n",
        "- **Anthropic**: $5 free credit (new accounts) - [Sign up](https://console.anthropic.com/)\n",
        "- **Google Gemini**: Free tier available - [Sign up](https://makersuite.google.com/)\n",
        "- **Hugging Face**: Free inference API - [Sign up](https://huggingface.co/)\n",
        "\n",
        "**Estimated Costs for This Demo:**\n",
        "- **OpenAI GPT-3.5-turbo**: ~$0.01-0.02 per test\n",
        "- **Anthropic Claude Haiku**: ~$0.01-0.02 per test  \n",
        "- **Google Gemini Pro**: ~$0.01-0.02 per test\n",
        "- **Hugging Face**: Free (local inference)\n",
        "\n",
        "**üí° Pro Tip**: Start with simulation mode (no keys required) to understand the concepts, then add your API key to see real LLM integration!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display Aletheia facet scores visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate facet scores (using the last test result)\n",
        "facets = {\n",
        "    \"Transparency\": 0.82,\n",
        "    \"Security\": 0.76,\n",
        "    \"Compliance\": 0.91,\n",
        "    \"Reliability\": 0.88,\n",
        "    \"Ethics\": 0.80,\n",
        "    \"Sustainability\": 0.73\n",
        "}\n",
        "\n",
        "# Create horizontal bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['#2E8B57', '#4169E1', '#DC143C', '#FF8C00', '#8A2BE2', '#20B2AA']\n",
        "bars = plt.barh(list(facets.keys()), list(facets.values()), color=colors, alpha=0.8)\n",
        "\n",
        "# Customize the chart\n",
        "plt.title(\"Aletheia Runtime Facet Scores (Simulated)\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Score (0‚Äì1.0)\", fontsize=12)\n",
        "plt.xlim(0, 1.0)\n",
        "\n",
        "# Add score labels on bars\n",
        "for i, (facet, score) in enumerate(facets.items()):\n",
        "    plt.text(score + 0.01, i, f\"{score:.2f}\", va='center', fontweight='bold')\n",
        "\n",
        "# Add threshold line\n",
        "plt.axvline(x=0.8, color='red', linestyle='--', alpha=0.7, label='Governance Threshold')\n",
        "plt.legend()\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nüìä Facet Score Summary:\")\n",
        "print(f\"Average Score: {sum(facets.values()) / len(facets):.3f}\")\n",
        "print(f\"Highest Score: {max(facets.values()):.3f} ({max(facets, key=facets.get)})\")\n",
        "print(f\"Lowest Score: {min(facets.values()):.3f} ({min(facets, key=facets.get)})\")\n",
        "print(f\"Above Threshold (0.8): {sum(1 for score in facets.values() if score >= 0.8)}/{len(facets)} facets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Educational explanation and summary\n",
        "print(\"üéì IOA Aletheia Runtime Governance - Educational Summary\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"This runtime simulation demonstrates how IOA enforces the 6 Aletheia facets at runtime.\")\n",
        "print(\"No API keys are required. The governance score is computed using mock evidence and facet weightings.\")\n",
        "print()\n",
        "print(\"üîç What You've Learned:\")\n",
        "print(\"‚Ä¢ How IOA Core integrates with Aletheia Framework v2.0 for runtime governance\")\n",
        "print(\"‚Ä¢ Real-time policy enforcement across 6 key facets: Transparency, Security, Compliance, Reliability, Ethics, Sustainability\")\n",
        "print(\"‚Ä¢ Evidence bundle generation for audit trails and compliance reporting\")\n",
        "print(\"‚Ä¢ Weighted scoring system that combines multiple governance dimensions\")\n",
        "print()\n",
        "print(\"üèóÔ∏è Technical Implementation:\")\n",
        "print(\"‚Ä¢ Policy-driven architecture with configurable thresholds\")\n",
        "print(\"‚Ä¢ Real-time decision making with cryptographic evidence\")\n",
        "print(\"‚Ä¢ Integration with existing Aletheia assessment workflows\")\n",
        "print(\"‚Ä¢ Scalable governance for production AI systems\")\n",
        "print()\n",
        "print(\"üìö Next Steps:\")\n",
        "print(\"‚Ä¢ Explore the full IOA Core documentation: https://github.com/OrchIntel/ioa-core\")\n",
        "print(\"‚Ä¢ Learn about Aletheia Framework: https://www.rolls-royce.com/innovation/the-aletheia-framework.aspx\")\n",
        "print(\"‚Ä¢ Join the community discussion: https://discord.gg/Fzxa5GG9\")\n",
        "print()\n",
        "print(\"‚öñÔ∏è Attribution:\")\n",
        "print(\"Aletheia Framework v2.0 ¬© Rolls-Royce Civil Aerospace. Licensed under CC BY-ND 4.0 International.\")\n",
        "print(\"IOA Core ¬© 2025 OrchIntel Systems Ltd. Licensed under Apache License 2.0.\")\n",
        "print()\n",
        "print(\"üéâ Thank you for exploring IOA Aletheia Runtime Governance!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
