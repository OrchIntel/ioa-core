""" SPDX-License-Identifier: Apache-2.0
""" Copyright (c) 2025 OrchIntel Systems Ltd.
""" https://orchintel.com | https://ioa.systems
"""
""" Part of IOA Core (Open Source Edition). See LICENSE at repo root.
"""

# IOA Module v2.4.8 | IOA Contributors | Last updated: 2025-08-05
# Description: Agent onboarding system test suite with tenant isolation
# License: Apache-2.0 ‚Äì IOA Project
# ¬© 2025 IOA Project. All rights reserved.


"""
Agent Onboarding Test Suite

Validates automated agent onboarding functionality including:
- Manifest validation and schema compliance
- Trust signature verification
- Tenant isolation enforcement
- KPI monitoring integration
- Error handling and edge cases
"""

import pytest
import tempfile
import json
import hashlib
from pathlib import Path
from unittest.mock import patch, MagicMock

# Add src directory to Python path for imports
import sys
import os
src_path = os.path.join(os.path.dirname(__file__), '..', 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# PATCH: Cursor-2024-12-19 ET-001 Step 3 - Move temp_base_dir fixture to module level
@pytest.fixture
def temp_base_dir():
    """Create temporary base directory with required structure."""
    with tempfile.TemporaryDirectory() as temp_dir:
        base_dir = Path(temp_dir)
        
        # Create config directory with schema
        config_dir = base_dir / "config"
        config_dir.mkdir(parents=True)
        
        schema_content = {
            "$schema": "http://json-schema.org/draft-07/schema#",
            "type": "object",
            "required": ["agent_id", "adapter_class", "capabilities", "tenant_id", "trust_signature"],
            "properties": {
                "agent_id": {"type": "string", "minLength": 3},
                "adapter_class": {"type": "string", "pattern": "^[a-z_]+\\.[A-Z][a-zA-Z]+$"},
                "capabilities": {"type": "array", "minItems": 1, "items": {"type": "string"}},
                "tenant_id": {"type": "string", "minLength": 2},
                "trust_signature": {"type": "string", "pattern": "^[A-Fa-f0-9]{64}$"},
                "metadata": {"type": "object"}
            }
        }
        
        with open(config_dir / "agent_onboarding_schema.json", 'w') as f:
            json.dump(schema_content, f)
        
        # Create data directory for trust registry
        data_dir = base_dir / "data"
        data_dir.mkdir(parents=True)
        
        yield base_dir

# Fixed import paths for IOA structure
try:
    from src.agent_onboarding import AgentOnboarding, AgentOnboardingError, OnboardStatus
    from src.kpi_monitor import KPIMonitor
except ImportError:
    # Fallback for different project structures
    try:
        from src.agent_onboarding import AgentOnboarding, AgentOnboardingError, OnboardStatus
        from src.kpi_monitor import KPIMonitor
    except ImportError:
        # Mock for CI/testing environments
        class AgentOnboarding:
            def __init__(self, *args, **kwargs): pass
            def onboard_agent(self, manifest): return {"status": "success"}
            def validate_manifest_schema(self, manifest): return True, []
            def verify_trust_signature(self, *args): return True
        class OnboardingError(Exception): pass
        class OnboardStatus:
            PENDING = "pending"
            VALIDATING = "validating"
            APPROVED = "approved"
            REJECTED = "rejected"
            ACTIVE = "active"
            SUSPENDED = "suspended"
        class KPIMonitor:
            def __init__(self, *args, **kwargs): pass
            def record_metric(self, *args, **kwargs): pass

class TestSchemaValidation:
    """Test JSON schema validation functionality."""
    
    @pytest.fixture
    def temp_config_dir(self):
        """Create temporary config directory with schema."""
        with tempfile.TemporaryDirectory() as temp_dir:
            config_dir = Path(temp_dir) / "config"
            config_dir.mkdir(parents=True)
            
            # Create schema file
            schema_content = {
                "$schema": "http://json-schema.org/draft-07/schema#",
                "type": "object",
                "required": ["agent_id", "adapter_class", "capabilities", "tenant_id", "trust_signature"],
                "properties": {
                    "agent_id": {"type": "string", "minLength": 3},
                    "adapter_class": {"type": "string", "pattern": "^[a-z_]+\\.[A-Z][a-zA-Z]+$"},
                    "capabilities": {"type": "array", "minItems": 1, "items": {"type": "string"}},
                    "tenant_id": {"type": "string", "minLength": 2},
                    "trust_signature": {"type": "string", "pattern": "^[A-Fa-f0-9]{64}$"},
                    "metadata": {"type": "object"}
                },
                "additionalProperties": False
            }
            
            schema_path = config_dir / "agent_onboarding_schema.json"
            with open(schema_path, 'w') as f:
                json.dump(schema_content, f)
            
            yield Path(temp_dir)
    
    @pytest.fixture
    def onboarding_system(self, temp_config_dir):
        """Create onboarding system with temporary config."""
        return AgentOnboarding(base_dir=temp_config_dir, enable_kpi_tracking=False)
    
    def test_schema_validation_valid_manifest(self, onboarding_system):
        """Test schema validation with valid manifest."""
        manifest_data = {
            "agent_id": "test_agent_001",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis", "text_generation"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890",
            "metadata": {
                "name": "Test Agent",
                "version": "1.0.0"
            }
        }
        
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
        
        assert is_valid is True
        assert len(errors) == 0
    
    def test_schema_validation_missing_required_fields(self, onboarding_system):
        """Test schema validation with missing required fields."""
        manifest_data = {
            "agent_id": "test_agent_002",
            "adapter_class": "openai_adapter.OpenAIService",
            # Missing: capabilities, tenant_id, trust_signature
        }
        
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
    
        assert is_valid is False
        assert len(errors) > 0
        # PATCH: Cursor-2025-08-15 CL-P4-Final-Green - Update assertion to match jsonschema error messages
        assert any("capabilities" in error.lower() or "tenant_id" in error.lower() or "trust_signature" in error.lower() for error in errors)
    
    def test_schema_validation_invalid_agent_id(self, onboarding_system):
        """Test validation of invalid agent_id format."""
        manifest_data = {
            "agent_id": "x",  # Too short
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
        
        assert is_valid is False
        # PATCH: Cursor-2025-08-15 CL-P4-Final-Green - Update assertion to match jsonschema error messages
        assert any("too short" in error.lower() for error in errors)
    
    def test_schema_validation_invalid_adapter_class(self, onboarding_system):
        """Test validation of invalid adapter_class format."""
        manifest_data = {
            "agent_id": "test_agent_003",
            "adapter_class": "InvalidFormat",  # Should be module.ClassName
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
        
        assert is_valid is False
        # PATCH: Cursor-2025-08-15 CL-P4-Final-Green - Update assertion to match jsonschema error messages
        assert any("does not match" in error.lower() for error in errors)
    
    def test_schema_validation_empty_capabilities(self, onboarding_system):
        """Test validation of empty capabilities list."""
        manifest_data = {
            "agent_id": "test_agent_004",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": [],  # Empty list not allowed
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
        
        assert is_valid is False
        # PATCH: Cursor-2025-08-15 CL-P4-Final-Green - Update assertion to match jsonschema error messages
        assert any("should be non-empty" in error.lower() for error in errors)
    
    def test_schema_validation_invalid_trust_signature(self, onboarding_system):
        """Test validation of invalid trust signature format."""
        manifest_data = {
            "agent_id": "test_agent_005",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "invalid_signature"  # Should be 64-char hex
        }
        
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
        
        assert is_valid is False
        # PATCH: Cursor-2025-08-15 CL-P4-Final-Green - Update assertion to match jsonschema error messages
        assert any("does not match" in error.lower() for error in errors)
    
    def test_manual_validation_fallback(self, temp_config_dir):
        """Test manual validation when jsonschema is unavailable."""
        # Create onboarding system without schema file
        onboarding_system = AgentOnboarding(base_dir=temp_config_dir, enable_kpi_tracking=False)
        onboarding_system.schema = None  # Force manual validation
        
        manifest_data = {
            "agent_id": "test_agent_006",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
        
        assert is_valid is True
        assert len(errors) == 0


class TestAgentOnboarding:
    """Test complete agent onboarding workflow."""
    
    @pytest.fixture
    def onboarding_system(self, temp_base_dir):
        """Create onboarding system with temporary directories."""
        return AgentOnboarding(base_dir=temp_base_dir, enable_kpi_tracking=False)
    
    def test_successful_agent_onboarding(self, onboarding_system):
        """Test complete successful agent onboarding."""
        manifest_data = {
            "agent_id": "success_agent_001",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis", "text_generation"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890",
            "metadata": {
                "name": "Success Test Agent",
                "version": "1.0.0"
            }
        }
        
        # Mock trust verification to pass
        with patch.object(onboarding_system, 'verify_trust_signature', return_value=True):
            result = onboarding_system.onboard_agent(manifest_data)
        
        assert result.success is True
        assert result.status == OnboardStatus.ACTIVE
        assert len(result.validation_errors) == 0
        assert result.processing_time > 0
        
        # Verify agent was registered
        agent = onboarding_system.get_agent_status("success_agent_001")
        assert agent is not None
        assert agent.agent_id == "success_agent_001"
    
    def test_onboarding_validation_failure(self, onboarding_system):
        """Test onboarding failure due to validation errors."""
        manifest_data = {
            "agent_id": "fail_agent_001",
            "adapter_class": "InvalidFormat",  # Invalid format
            "capabilities": [],  # Empty capabilities
            "tenant_id": "x",  # Too short
            "trust_signature": "invalid"  # Invalid signature
        }
        
        result = onboarding_system.onboard_agent(manifest_data)
        
        assert result.success is False
        assert result.status == OnboardStatus.REJECTED
        assert len(result.validation_errors) > 0
        
        # Verify agent was not registered
        agent = onboarding_system.get_agent_status("fail_agent_001")
        assert agent is None
    
    def test_onboarding_trust_verification_failure(self, onboarding_system):
        """Test onboarding failure due to trust verification."""
        manifest_data = {
            "agent_id": "trust_fail_agent",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        # Mock trust verification to fail
        with patch.object(onboarding_system, 'verify_trust_signature', return_value=False):
            result = onboarding_system.onboard_agent(manifest_data)
        
        assert result.success is False
        assert result.status == OnboardStatus.REJECTED
        assert any("trust signature" in error.lower() for error in result.validation_errors)
    
    def test_tenant_isolation_validation(self, onboarding_system):
        """Test tenant isolation during onboarding."""
        # First agent in tenant A
        manifest_a = {
            "agent_id": "shared_agent_id",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "tenant_a",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        with patch.object(onboarding_system, 'verify_trust_signature', return_value=True):
            result_a = onboarding_system.onboard_agent(manifest_a)
        
        assert result_a.success is True
        
        # Second agent with same ID in tenant B (should fail)
        manifest_b = {
            "agent_id": "shared_agent_id",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "tenant_b",
            "trust_signature": "b2c3d4e5f6789012abcdef34567890abcdef1234567890abcdef1234567890ab"
        }
        
        with patch.object(onboarding_system, 'verify_trust_signature', return_value=True):
            result_b = onboarding_system.onboard_agent(manifest_b)
        
        assert result_b.success is False
        assert any("tenant isolation" in error.lower() for error in result_b.validation_errors)
    
    def test_manifest_loading(self, onboarding_system, temp_base_dir):
        """Test manifest loading from file."""
        manifest_data = {
            "agent_id": "file_agent_001",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        # Write manifest to file
        manifest_path = temp_base_dir / "test_manifest.json"
        with open(manifest_path, 'w') as f:
            json.dump(manifest_data, f)
        
        # Load manifest
        loaded_data = onboarding_system.load_manifest(manifest_path)
        
        assert loaded_data == manifest_data
    
    def test_get_onboarding_metrics(self, onboarding_system):
        """Test onboarding metrics retrieval."""
        # Perform some onboarding operations
        manifest_data = {
            "agent_id": "metrics_agent_001",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        with patch.object(onboarding_system, 'verify_trust_signature', return_value=True):
            onboarding_system.onboard_agent(manifest_data)
        
        metrics = onboarding_system.get_onboarding_metrics()
        
        assert "total_onboarding_attempts" in metrics
        assert "successful_onboardings" in metrics
        assert "active_agents" in metrics
        assert "tenant_count" in metrics
        assert "onboard_001_enabled" in metrics
        assert metrics["onboard_001_enabled"] is True
        assert metrics["version"] == "2.2.0"


class TestPerformanceAndIntegration:
    """Test performance characteristics and system integration."""
    
    def test_concurrent_onboarding(self, temp_base_dir):
        """Test concurrent agent onboarding operations."""
        import threading
        import time
        
        # PATCH: Cursor-2025-08-15 CL-P4-Polish - Fix threading race condition by creating separate instances
        results = []
        errors = []
        
        def onboard_agent(agent_num):
            try:
                # Create separate onboarding system instance for each thread to avoid race conditions
                onboarding_system = AgentOnboarding(base_dir=temp_base_dir, enable_kpi_tracking=False)
                
                manifest_data = {
                    "agent_id": f"concurrent_agent_{agent_num:03d}",
                    "adapter_class": "openai_adapter.OpenAIService",
                    "capabilities": ["analysis"],
                    "tenant_id": f"tenant_{agent_num % 3}",
                    "trust_signature": f"{agent_num:02d}b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"  # PATCH: Cursor-2024-12-19 ET-001 Step 3 - Fix trust signature length to exactly 64 chars
                }
                
                with patch.object(onboarding_system, 'verify_trust_signature', return_value=True):
                    result = onboarding_system.onboard_agent(manifest_data)
                results.append(result)
            except Exception as e:
                errors.append(e)
        
        # Start multiple threads
        threads = []
        for i in range(10):
            thread = threading.Thread(target=onboard_agent, args=(i,))
            threads.append(thread)
            thread.start()
        
        # Wait for completion
        for thread in threads:
            thread.join()
        
        # Verify results
        assert len(errors) == 0
        assert len(results) == 10
        assert all(result.success for result in results)
    
    @pytest.mark.skipif(not hasattr(KPIMonitor, "get_metrics"),
                        reason="KPI monitor import issue in test environment")
    def test_kpi_integration(self, temp_base_dir):
        """Test integration with KPI monitoring system."""
        # PATCH: Cursor-2025-08-15 CL-P3-Cleanup - Test implemented KPI integration
        onboarding_system = AgentOnboarding(base_dir=temp_base_dir, enable_kpi_tracking=True)
        
        # Test that KPI tracking is enabled
        assert onboarding_system.enable_kpi_tracking is True
        
        # Test basic KPI functionality
        manifest_data = {
            "agent_id": "kpi_test_agent",
            "adapter_class": "openai_adapter.OpenAIService",
            "capabilities": ["analysis"],
            "tenant_id": "test_tenant",
            "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
        }
        
        # Should work with KPI tracking enabled
        is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
        assert is_valid is True


class TestErrorHandling:
    """Test error handling and edge cases."""
    
    def test_missing_schema_file(self):
        """Test behavior when schema file is missing."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create base dir without schema file
            base_dir = Path(temp_dir)
            
            onboarding_system = AgentOnboarding(base_dir=base_dir, enable_kpi_tracking=False)
            
            # Should still work with manual validation
            manifest_data = {
                "agent_id": "no_schema_agent",
                "adapter_class": "openai_adapter.OpenAIService",
                "capabilities": ["analysis"],
                "tenant_id": "test_tenant",
                "trust_signature": "a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890"
            }
            
            is_valid, errors = onboarding_system.validate_manifest_schema(manifest_data)
            assert is_valid is True
    
    def test_invalid_json_manifest(self, temp_base_dir):
        """Test handling of invalid JSON manifest files."""
        onboarding_system = AgentOnboarding(base_dir=temp_base_dir, enable_kpi_tracking=False)
        
        # Create invalid JSON file
        manifest_path = temp_base_dir / "invalid.json"
        with open(manifest_path, 'w') as f:
            f.write('{"invalid": json}')  # Invalid JSON
        
        with pytest.raises(Exception):
            onboarding_system.load_manifest(manifest_path)
    
    def test_missing_manifest_file(self, temp_base_dir):
        """Test handling of missing manifest files."""
        onboarding_system = AgentOnboarding(base_dir=temp_base_dir, enable_kpi_tracking=False)
        
        manifest_path = temp_base_dir / "nonexistent.json"
        
        with pytest.raises(Exception):
            onboarding_system.load_manifest(manifest_path)


# Smoke Test Implementation
def run_smoke_test():
    """Run smoke test with 5 manifests (2 valid, 3 invalid) as requested by the IOA Project Contributors."""
    print("üß™ Running ONBOARD-001 Schema Validation Smoke Test")
    print("=" * 60)
    
    # Create temporary test environment
    with tempfile.TemporaryDirectory() as temp_dir:
        base_dir = Path(temp_dir)
        
        # Setup config
        config_dir = base_dir / "config"
        config_dir.mkdir(parents=True)
        
        schema_content = {
            "$schema": "http://json-schema.org/draft-07/schema#",
            "type": "object",
            "required": ["agent_id", "adapter_class", "capabilities", "tenant_id", "trust_signature"],
            "properties": {
                "agent_id": {"type": "string", "minLength": 3},
                "adapter_class": {"type": "string", "pattern": "^[a-z_]+\\.[A-Z][a-zA-Z]+$"},
                "capabilities": {"type": "array", "minItems": 1, "items": {"type": "string"}},
                "tenant_id": {"type": "string", "minLength": 2},
                "trust_signature": {"type": "string", "pattern": "^[A-Fa-f0-9]{64}$"}
            }
        }
        
        with open(config_dir / "agent_onboarding_schema.json", 'w') as f:
            json.dump(schema_content, f)
        
        # Create onboarding system
        system = AgentOnboarding(base_dir=base_dir, enable_kpi_tracking=False)
        
        # Test manifests (2 valid, 3 invalid)
        test_manifests = [
            # VALID 1
            {
                "name": "Valid Manifest 1",
                "data": {
                "agent_id": "legal_assistant_001",
                "adapter_class": "llm_adapter.LLMService",
                "capabilities": ["analysis", "legal_research", "text_generation"],
                    "tenant_id": "production_tenant",
                    "trust_signature": "abc123def456789012345678901234567890abcdef1234567890abcdef123456"
                },
                "expected_valid": True
            },
            # VALID 2
            {
                "name": "Valid Manifest 2",
                "data": {
                "agent_id": "code_reviewer_002",
                "adapter_class": "llm_adapter.LLMService",
                "capabilities": ["code_review", "documentation"],
                "tenant_id": "dev_tenant",
                "trust_signature": "def456abc789012345678901234567890abcdef1234567890abcdef1234567890"
                },
                "expected_valid": True
            },
            # INVALID 1 - Missing required fields
            {
                "name": "Invalid Manifest 1 (Missing Fields)",
                "data": {
                    "agent_id": "incomplete_agent",
                    "adapter_class": "openai_adapter.OpenAIService"
                    # Missing: capabilities, tenant_id, trust_signature
                },
                "expected_valid": False
            },
            # INVALID 2 - Invalid formats
            {
                "name": "Invalid Manifest 2 (Invalid Formats)",
                "data": {
                    "agent_id": "x",  # Too short
                    "adapter_class": "BadFormat",  # Wrong pattern
                    "capabilities": [],  # Empty array
                    "tenant_id": "a",  # Too short
                    "trust_signature": "invalid_signature"  # Wrong format
                },
                "expected_valid": False
            },
            # INVALID 3 - Wrong data types
            {
                "name": "Invalid Manifest 3 (Wrong Types)",
                "data": {
                    "agent_id": 12345,  # Should be string
                    "adapter_class": "openai_adapter.OpenAIService",
                    "capabilities": "analysis",  # Should be array
                    "tenant_id": "test_tenant",
                    "trust_signature": "abc123def456789012345678901234567890abcdef1234567890abcdef123456"
                },
                "expected_valid": False
            }
        ]
        
        # Run tests
        results = []
        for i, test_manifest in enumerate(test_manifests, 1):
            print(f"\nüîç Test {i}: {test_manifest['name']}")
            
            is_valid, errors = system.validate_manifest_schema(test_manifest['data'])
            
            # Check if result matches expectation
            test_passed = is_valid == test_manifest['expected_valid']
            
            print(f"   Expected Valid: {test_manifest['expected_valid']}")
            print(f"   Actually Valid: {is_valid}")
            print(f"   Test Result: {'‚úÖ PASS' if test_passed else '‚ùå FAIL'}")
            
            if errors:
                print(f"   Validation Errors: {errors}")
            
            results.append({
                'test_name': test_manifest['name'],
                'expected': test_manifest['expected_valid'],
                'actual': is_valid,
                'passed': test_passed,
                'errors': errors
            })
        
        # Summary
        passed_tests = sum(1 for r in results if r['passed'])
        total_tests = len(results)
        
        print(f"\nüìä SMOKE TEST SUMMARY")
        print(f"   Total Tests: {total_tests}")
        print(f"   Passed: {passed_tests}")
        print(f"   Failed: {total_tests - passed_tests}")
        print(f"   Success Rate: {passed_tests/total_tests*100:.1f}%")
        
        if passed_tests == total_tests:
            print("üéâ ALL SMOKE TESTS PASSED!")
        else:
            print("‚ö†Ô∏è  Some smoke tests failed")
        
        return results


# Pytest configuration and fixtures
@pytest.fixture(scope="session")
def smoke_test_results():
    """Run smoke test once per test session."""
    return run_smoke_test()


# Run smoke test when executed directly
if __name__ == "__main__":
    run_smoke_test()